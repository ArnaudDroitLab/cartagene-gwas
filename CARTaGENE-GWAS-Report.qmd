---
title: "GWAS Analyses utilising the CARTaGENE database"
author: "Bastien CHASSAGNOL and Marie-Pier and Clément and Fabrice"
date: last-modified	
number-sections: true
engine: knitr
toc: true
toc-depth: 4
lang: en-GB
bibliography: Cartagene.bib
link-citations: true
highlight-style: github
filters:
  - highlight-text
# code options
execute:
  message: false
  warning: false
  error: false
  eval: true
  echo: true
# Table options
tbl-cap-location: bottom
format: 
  html:
    embed-resources: true
    toc-location: left
    toc-expand: 3
    theme:
      light: cosmo
      dark: cosmo
    sidebar: true
    lightbox: true
    comments: 
      hypothesis: true
    # code options
    code-fold: show
    code-link: true
    code-annotations: hover
    page-layout: full
    collapse: true
  docx:
    toc-title: Contents
editor: source
---

# Reproduce the pipeline and questions to address

## Two-step pipeline

Assuming PCA components have already been computed (report to @sec-white-selection and @sec-preprocessing-redundancy for details), follow this two-step protocol:

i.  Retrieve all known variants associated with your genes of interest in @sec-variant-extraction, and generate the corresponding `VCF` file.
ii. Change the response variable to predict in @lst-glm-bash (with option `--glm`, the model applied should adjust automatically whether the outcome is binary or continuous).

## Reproducibility

Two command-lines tools are needed for reproducing the analyses and/or delivering new outcomes:

-   [`plink2`](https://s3.amazonaws.com/plink2-assets/alpha6/plink2_linux_x86_64_20250129.zip) for running most of the SNPs analyses.
-   [`bcftools`](https://github.com/samtools/bcftools/releases/download/1.21/bcftools-1.21.tar.bz2) for extracting variants of interest in a given genomic region.
-   Details for installing from scratch these two CLI tools are reported in @sec-tools-install.

The code, main results (figures and tables), along with reports, are versioned in an unique place, on `GitHub`: 

-   [`GitHub` project: `cartagene-gwas`](https://github.com/ArnaudDroitLab/cartagene-gwas) to version the code
-   R project + `renv()` to create a reproducible and standalone computational environment.

## Important Questions to Address!! {#sec-questions}

-   [Check Human reference genome assembly, see @sec-HDAC9. Indeed, among the five genome arrays assembled in @sec-white-selection, most have been mapped against `GRCh37 (hg19)`, but at least one has been mapped against the recent `hg38 (GRCh38, 2013)` versio, report to page 38 in @pelletier2022.]{fg="red"} 
- We definitely do not have the most recent version of `CarTaGene` (in the paper, around $30000$ samples mentioned, against $20000$ in my database + )
-   [Why starting from BAM/BED/BIM pre-processed files in @sec-white-selection, instead of `FASTQ` files available in `/mnt/projects_tn01/Cartagene/data/merged`? Related question: we use the `BAM/BED/BIM` files stored in folder `/mnt/projects_tn01/Cartagene/analyses/QC` for PCA computation in @sec-preprocessing-redundancy, but `VCF` files stored in `/mnt/projects_tn01/Cartagene/analyses/variants_extraction` for running the GWAS analyses, for which reason?]{fg="red"}
-   [Why use a biased *gene-centric approach*, instead of true GWAS (further discussed in @sec-post-GWAS)?]{fg="red"}. Specifically, in constrast with the conclusions of this limited study on a single gene, namely `HDAC-9`, running a whole GWAS analysis on CarTaGene revealed promising and significant SNPs, as illustrated in @fig-gwas-global. 
    - An alternative to *candidate gene* association studies relies on the *linkage analysis approach*, which leverages familial kinships to map genetic variants underlying common human diseasess. **This strategy requires both genetic data and detailed family pedigrees.**, report to for details.

- Address intrinsic heteoreigninity of the sampled population, see [A benchmark study on current GWAS models in admixed populations](https://doi.org/10.1093/bib/bbad437), from @yang2024bib. 

[**GWAS of `CarTaGene` SNPs using TOPMed Merge-Impute (first, merge all genotype arrays, then impute SNPs) strategy**.  obtained through genotyping and imputation using the TOPMed Imputation Server under two analytic workflows: Impute-Merge and Merge-Impute. Manhattan plots depict the genome-wide association study (GWAS) results for variants (*Single nucleotide polymorphisms* and *insertion-deletion*), before and after imputation. The $y$-axis represents the $–\log_{10}(p)$ for the association of each variant. [Variants with $p$-values equal to zero are highlighted in green]{fg="green"}, while the [horizontal red line indicates the genome-wide *significance threshold*.]{fg="red"}. The total sample consisted of $27,422$ individuals.](./figures/global-GWAS.png){#fig-gwas-global}


# Introduction

## GWAS analyses: Concepts

- Fundamental paper, aka [Genome-wide association studies: theoretical and practical concerns](https://www.nature.com/articles/nrg1522), from @wang2005nrg.

- Book reviewing pros and cons of statistical GWAS approaches, from [Overview of Statistical Methods for Genome-Wide Association Studies](https://doi.org/10.1007/978-1-62703-447-0_6), from @hayes2013gasagp.

- [Genome-wide association studies](https://www.nature.com/articles/s43586-021-00056-9), from @uffelmann2021nrmp.

## Original design

- [Cohort profile of the `CARTaGENE` study: Quebec’s population-based biobank for public health and personalized genomics](https://doi.org/10.1093/ije/dys160), from @awadalla2013ijoe. 

## Impute unobserved GWAS in `CARTaGENE` 

- Most of the `CARTaGENE` samples are *WES* (whole exome sequencing), instead of most complete *WGS* (whole genome sequencing).

- [Évaluation de l’imputation des données génétiques Canadiennes-Françaises](https://umontreal.scholaris.ca/bitstreams/83d64aa1-da20-4a6f-8ae0-9dcb0165417d/download), from @pelletier2022. **Key points**:
  - **Introduction section**: presents the most critical issues when dealing with GWAS information: linkage disequilibrium, 
  - Paper, corresponding to Chapter 3, named "Evaluation of genetic imputation in the French-Canadian founder population", pages 54-93. Details the underlying imputation strategy, derived from TOPMed (for Trans-Omics for Precision Medicine). 
  - Page 61: Describes the imputation strategy, along with the preprocessing steps. Code for reproducing the analyses is [FC-imputation](https://github.com/JustinPelletier/FC-imputation), kinda messy I must admit. **Imputation performance** is much lower for SNPs exhibiting `MAF<0.01`, strongly promoting early discarding.  
  - **Page 99**: Strong polymorphism in some regions, such as `HLA`.
  - [Reach out [Julie Hussin](mailto:julie.hussin@umontreal.ca), or [LinkedIn profile](https://www.linkedin.com/in/hussinju/)
, who actually ran the PCA computations and global GWAS analyses.]{fg="red"}


# Analyses

## Setup enviroment

### Bash configuration:

We use *symbolic links* to encapsulate the whole project, see [`ln -s` Post](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_the-most-underrated-unix-command-for-bioinformatics-activity-7287842704753848321-qhe) for details:

```{bash}
# Render executive files findable
export PATH=./bin/bcftools/bin:$PATH
export PATH=./bin/plink2:$PATH

# Check versions
plink2 --version
echo -e "\n"
bcftools --version

# Create symbolic links to organise everythin within the same folder
# warning: if original content is deleted, everything broke!!!!!!
echo ln -s ./data/genotypages/ /mnt/projects_tn01/Cartagene/genotypage/imputation/imputation_merged
```

### R configuration:

```{r}
#| label: setup
# data wrangling and visualisations
library(haven)
library(flextable)
library(dplyr)
library(ggplot2)
# Required for code linking
library(downlit)
library(xml2)

# Retrieve gene positions
library(org.Hs.eg.db)  # Provides gene symbol to Entrez ID mapping

# for generating nice visualisations
source("R/gwas_plots.R")
```

## Compute PCA **loadings**

### Step 1: Merge genotype arrays {#sec-white-selection}

Genotypes were split into five different genotyping arrays. Arrays were merged together using Bash script [merge_datasets.sh](/mnt/projects_tn01/Cartagene/analyses/QC/merge_datasets.sh), and command `--bmerge` of tool `plink2`. This command only keeps matching **SNPs** and **alleles** across the 5 arrays.

The resulting `BAM` files (sequence alignments), `BED` files (genomic regions of interest, for viewers), `BIM` (SNP information) and `FAM` (phenotype data, such as individuals and pedigree), are listed in folder `/mnt/projects_tn01/Cartagene/analyses/QC`, with prefix `merge_5_*` (for 5 genotypes concatenated).

### Step 2: Preprocessing for Removing Correlated Features {#sec-preprocessing-redundancy}

**Remark**: the prefix `eur_only` stands for Caucasian phenotypes.

#### i) Trim missing SNPs {#sec-missing-SNPs}

-   **Inputs**:

    -   **Phenotype Ids of white individuals**: `eur_only/cartagene_self_reported_EUR.plink_format.txt`
    -   **BIM/BED/FAM/hh** folder generated with `--make-bed` command: `analyses/QC/merge_5_datasets`. [It seems that among the 5 genotype arrays concatenated, at least one genotype, namely `gsa.17k.final.hg19.bim,` has been mapped thanks to `GRCh37 (hg19)` reference instead of more recent `hg38 (GRCh38, 2013)` version.]{fg="red"}\]

-   **Objective**: Keep white individuals and remove SNPs variants with genotyping rate $<0.95$. The **genotyping rate** measures the proportion of successfully genotyped markers and indicates how complete the genotype data is. Removing SNPs with low Genotyping Rate increases Missing Data imputation performance, and overall maintains higher statistical power.

-   [**Remark**: Also advised to exclude samples with low Genotyping Rate, below 0.98.]{fg="red"}

-   **Bash command**:

``` bash
plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/merge_5_datasets \
  --geno 0.05 \
  --keep /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/cartagene_self_reported_EUR.plink_format.txt \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095
```

#### ii) Hardy-Weinberg Disequilibrium {#sec-HWD}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files generated from previous SNP missing removal, see @sec-missing-SNPs.

-   **Objective**: Remove SNPs variants with **Hardy-Weinberg disequilibrium** $< 1 \times 10^{-6}$. Indeed, if a SNP is in HWD, it might reflect hidden sub-population structure or poor-quality rather than true genetic associations.

-   [**Remark**: If a SNP is under strong **natural selection**, such as SNPs involved in the HLA genes, they are likely to deviate from HWE due to balancing selection. If the SNP is biologically important, don't exclude it blindly!! Alternative: compute the SNP score.]{fg="red"}

-   **Bash command**:

``` bash
plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095 \
  --hwe 1e-6 \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06
```

#### iii) Linkage Disequilibrium {#sec-LD}

##### Local LD analysis {#sec-local-LD}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `/merge_5_datasets.eur_only.geno095.hwe1e0` generated from previous HWD SNP trimming, see @sec-HWD.

-   **Objective**: Remove SNPs variants associated with strong **linkage disequilibrium**. In an ideal population under random mating, allele combinations should be independent ($r^2=0$). However, due to factors such as genetic drift, or physical proximity, certain alleles tend to be inherited together more often than expected. In GWAS, LD pruning avoids **overfitting** (SNPs in high LD carry redundant information), avoiding spurious inflation of GWAS association signals.

-   [**Remark**: Report the Number of SNPs Before and After Pruning, usually more stringent $R^2$ is considered.]{fg="red"}

-   **Tool**: `plink --indep-pairwise 50 5 0.5` will discard SNPs with a correlation coefficient above $0.5$ (given that a $R^2$ score of 1 indicates a perfect correlation), on a *rolling window* of $50$ SNPs (focus on local zones) and step $5$ by $5$.

-   **Bash command**:

``` bash
# identify SNPs with low LD
plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06 \
  --indep-pairwise 50 5 0.5 \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps
  
# save pruned SNPs associated with low LD
plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06 \
  --extract /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.prune.in \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile
```

##### Global LD analysis from prior expert knowledge {#sec-global-LD}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile` generated from previous local LD trimming, see @sec-local-LD.

-   **Objective**: Remove SNPs variants associated with strong **linkage disequilibrium** using prior expert knowledge, see [here](http://dougspeed.com/wp-content/uploads/highld.txt) for details.[^2]

[^2]: Notably includes MHC, lactase region, and known inversions `8p23` and `17q21.31`. **Haplotypes**?

```{=html}
<!-- -->
```
-   **Bash command**:

``` bash

plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile \
  --exclude range /mnt/projects_tn01/Cartagene/analyses/QC/high_ld_regions.plink_format.txt \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded
```

#### iv) Exclude affiliated individuals based on high IBD scores

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile` generated from previous local LD trimming, see @sec-local-LD.

-   **Objective**: Exclude related individuals computing **identity by descent** (IBD), a genetic metric of the relatedness between two individuals, and exclude one individual by pairs of individuals with a score above $0.2$. Indeed, strongly associated patients increase bias in GWAS, raising a stronger score than expected in the general population, and prevents quality controls to detect duplicates or sample mix-ups.

-   **Details**: A two-step, more stringent IBD filtering strategy has been chosen, eliminating *first* the most problematic individuals, and the *second* ensuring the remaining related individuals are properly filtered (for each correlated pair, prune randomly one of them). The Bash instructions are reported [here](/mnt/projects_tn01/Cartagene/analyses/QC/eur_only/related_ids/Readme.txt).

-   [**Remarks**: Choice of a heuristic threshold of $n=68$ affiliated patients removal to be further discussed. Current score of `PI_HAT` score of $0.2$ is surprising, as common thresholds are either $0.25$ for discarding grandparent-grandchild, or $0.125$ or lower for only keeping the most distant relatives. Starting from `PLINK 2.0`, the recommended approach is now utilizing the `--king-cutoff` command, over the older `--rel-cutoff` and `--genome --min` + aggregates all subsequent steps simultaneously, especially in heterogeneous populations]{fg="red"}[^3].

-   **Bash command**:

[^3]: [`--king-cutoff 0.0884` corresponds to `PI_HAT = 0.125`, see details [here](http://biostars.org/p/434832/#434898)]{fg="red"}

``` bash

# Calculate IBD, select individuals with IBD above 0.2 for further pruning

plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile \
  --genome \
  --memory 12006 \
  --min 0.2 \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.IBD


# Remove related individuals, in a two-stage process

plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_1_of_2 \
  --remove /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/related_ids/IBD.genome.iids.merged.sorted.count.reverse.ids_related_to_2_individuals.ids_only.FID_IID_format


plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_1_of_2 \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_2_of_2 \
  --remove /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/related_ids/IBD.genome2.related_individuals_to_remove.txt 
```

### Step 3: PCA {#sec-PCA}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded` resulting from the pre-processing operations reported in @sec-preprocessing-redundancy (LD, HWD and missing SNPs trimming)
    -   **phenotype IDs** listed in `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_2_of_2`, where all affiliated patients have been removed.

-   **Objective**: Compute PCA and keep the 10 first principal components.

-   [**Remark**: use visualizations, such as *scree plots*, elbow point or/and *Tracy-Widom Test* to select the final number of PCs, instead of hard threshold, and on the other hand, scatter plots to identify latent structures. See details [here](https://www.linkedin.com/posts/joachim-schork_statistics-research-visualanalytics-activity-7251577056570785795-b6Rk) and [there](https://www.linkedin.com/feed/update/urn:li:activity:730919293617122508)]{fg="red"}.

-   **Bash command**:

``` bash

plink2 \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded \ 
  --keep merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_2_of_2.fam \
  --memory 12006 \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded.unrelated_ind.PCA \
  --pca 10
```

## Run GWAS analyses

### Step 1: Phenotype feature extraction {#sec-phenotype-extraction}

Original phenotype annotations are available in folder `/mnt/projects_tn01/Cartagene/analyses/phenotypes`. Variables of interest can e downloaded [here](./data/phenotypes/cartagene_response_variables.xlsx):

```{r}
#| label: tbl-cartage-pheno-features
#| tbl-cap: Features of interest, and response variables, to predict in `CarTaGene`.

cartagene_response_variables <- readxl::read_excel("data/phenotypes/cartagene_response_variables.xlsx")

flextable(cartagene_response_variables) |> 
  bold(part="header")

```

#### i) Merge genotype IDs, phenotypes and PCAs values

General phenotype features are provided in

```{r}
#| label: tbl-cartage-pheno
#| tbl-cap: Read SAS table describing CarTaGene phenotypes.

cartagene_phenodata <- haven::read_sas("data/phenotypes/cart_mars2025.sas7bdat")
flextable(head(cartagene_phenodata)) |> 
  bold(bold = TRUE, part = "header")

```

Number of patients overall is `{r} nrow(cartagene_phenodata)`, and number of phenotype variables is `{r} ncol(cartagene_phenodata)`.

We then need to map each individual patient ID (`IID`) with its corresponding genotype array [@tbl-cartage-genotypes-IDs], as done in @lst-join-phenotype-genotype.

```{r}
#| label: tbl-cartage-genotypes-IDs
#| tbl-cap: Read SAS table describing CarTaGene phenotypes.
cartagene_genotypes_ID <- readr::read_csv2("./data/phenotypes/cartagene_genotype_IDs.csv",
                                           show_col_types = FALSE, 
                                           col_types = c("d", "c","c")) |> 
  dplyr::rename(PROJECT_CODE = "project_code", geno_id = "file_111", batch="batch")
  

flextable(head(cartagene_genotypes_ID)) |> 
  bold(bold = TRUE, part = "header")
```

```{r}
#| label: join-phenotype-genotype
#| lst-label: lst-join-phenotype-genotype
#| lst-cap: Inner join between phenotypes IDs and genotypes, while constraing the remaining individuals to belong to white ethnicity. 
cartagene_phenodata <- cartagene_phenodata |> 
  dplyr::inner_join(cartagene_genotypes_ID, by="PROJECT_CODE") |> 
  dplyr::filter(ETHNICITY6M=="Blanc")
```

The resulting phenotype table, after joining with genotypes IDs and restraining to Eurasian phenotypes, stores `{r} nrow(cartagene_phenodata)` individuals.

PCA vectors computed in @sec-PCA are subsequently merged with phenotype data in @lst-join-phenotype-PCA.

```{r}
#| label: join-phenotype-PCA
#| lst-label: lst-join-phenotype-PCA
#| lst-cap: Inner join between phenotypes IDs and first 10 PCA eigen vectors.

PCs <- readr::read_delim("data/PCAs/PCA_eigenvec",
                        col_names = c("FID", "geno_id", "PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9", "PC10"), 
                        delim = " ",
                        show_col_types = FALSE)
cartagene_phenodata <- PCs |> 
  dplyr::inner_join(cartagene_phenodata, by="geno_id") |> 
  dplyr::rename(IID = geno_id)

```

-   [**Remark**: Avoid using French CSV settings, switch to universal convention, where delimiter is a comma: `,`]{fg="red"}

#### ii) Bone damage: osteoporosis and fractures

-   The response variables `OSTEOPONIA` and `OSTEOPOROSIS` have been computed following these rules [@lst-OSTEOPONIA-computation], with the resulting contingency tables reported in @tbl-OSTEOPONIA-computation.

    1.  Exclude patients with `osteosecondaire==1`.
    2.  `OSTEOPONIA` is `case: 1` if `DMOTSCORE_mod <= -1.5` and `control` elsewhere[^4].
    3.  `OSTEOPOROSIS` is `case: 1` if `DMOTSCORE_mod < -2.5` and `control` elsewhere. [`DMOTSCORE_mod` has been likely Gaussian standardised + lacks of evidence supporting clear multi-modal distributions. Is there a reporting of the pre-processing operations?]{fg="red"}.

[^4]: `DMOTSCORE_mod` represents the measurement of bone mineral density at the calcaneus. While no universally accepted threshold exists for the diagnosis of osteopenia or osteoporosis, we adopt the conventional cut-off of `-1.5`.

```{r}
#| label: tbl-OSTEOPONIA-computation
#| lst-label: lst-OSTEOPONIA-computation
#| lst-cap: Generate scores of interest.
#| tbl-cap: OSTEOPOROSIS is considered as more severe than `OSTEOPONIA`, hence the striclty lower number of individuals affected by the disease. 

cartagene_phenodata <- cartagene_phenodata |> 
  dplyr::mutate(DMOTSCORE_mod = dplyr::if_else(OSTEOSECONDAIRE ==1, NA, DMOTSCORE), 
                OSTEOPONIA = dplyr::case_when(DMOTSCORE_mod <= -1.5 ~ 1, 
                                              DMOTSCORE_mod > -1.5 ~ 0, 
                                              .default = NA),
                OSTEOPOROSIS = dplyr::case_when(DMOTSCORE_mod < -2.5 ~ 1, 
                                              DMOTSCORE_mod >= -2.5 ~ 0, 
                                              .default = NA)) 
  

flextable::proc_freq(cartagene_phenodata, 
                     "OSTEOPONIA", "OSTEOPOROSIS")

```

-   **Fracture events**:

    1.  We merge `FXALL_PRE5` (any fracture occurring within the five years prior to recruitment) with `FXALL_POST` (all fractures occurring during follow-up) into variable `FXALL`, being positive if an event fracture occured prior or posterior. [Which variable between `FXALL_PRE5` and `FXALL_PRE1` should be considered? Variables differ by `{r} length(which(cartagene_phenodata$FXALL_PRE5!=cartagene_phenodata$FXALL_PRE1)/nrow(cartagene_phenodata))` $\%$ overall!!]{fg="red"}
    2.  `FXMOF_POST` (only osteoporotic fractures during follow-up)[^5] [In contrast with labelling, osteoporotic events, as stored in `FXMOF_POST`, contain both pre-, and post- fracture events, as shown by @fig-venn-osteo.]{fg="red"}

[^5]: For both `FXMOF_POST` and `FXALL_POST`, the dates of occurrence and censoring dates are available in `FXALL_POSTDATE` and `FXMOF_POSTDATE` respectively.

```{r}
#| label: tbl-fracture-computation
#| tbl-cap: Description of fracture events.
#| tbl-subcap:
#|   - "All fractures vs osteporotic fractures"
#|   - "Pre vs post-fractures."
#| layout-ncol: 2

cartagene_phenodata <- cartagene_phenodata |> 
  mutate(FXALL = dplyr::if_else(FXALL_POST == 1L | FXALL_PRE1 ==1L, 1L, 0L))

flextable::proc_freq(cartagene_phenodata, 
                     "FXALL", "FXMOF_POST")

flextable::proc_freq(cartagene_phenodata, 
                     "FXALL_POST", "FXALL_PRE1")
```

```{r}
#| label: fig-venn-osteo
#| fig-cap: "Venn Diagrams of fracture events, before and after diagnosis control." 
#| fig-subcap:
#|   - "Generated by `VennDiagram`"
#|   - "Generated by `ggVennDiagram`" 
#| layout-ncol: 2
#| echo: false

labels_OSTEO <- list(`post fracture` = cartagene_phenodata$IID[cartagene_phenodata$FXALL_POST==1], 
                          `pre facture` = cartagene_phenodata$IID[cartagene_phenodata$FXALL_PRE1==1], 
                          `osteoporotic fracture` = cartagene_phenodata$IID[cartagene_phenodata$FXMOF_POST==1])

ggVennDiagram::ggVennDiagram(labels_OSTEO) +
  scale_x_continuous(expand = expansion(mult = .2)) +
  scale_fill_distiller(palette = "RdBu") +
  labs(title = "Venn Diagrams of fracture events",
       caption = Sys.Date())
# Chart
# VennDiagram::venn.diagram(
#         x = labels_OSTEO,
#         filename = 'figures/venn_diagramm_fractures.png',
#         output=FALSE,
#         # Output features
#         imagetype="png" ,
#         height = 480 , 
#         width = 480 , 
#         resolution = 300,
#         # Circles
#         lwd = 2,
#         lty = 'blank',
#         col=c("#440154ff", '#21908dff', '#fde725ff'),
#         fill = c(scales::alpha("#440154ff",0.3), scales::alpha('#21908dff',0.3), scales::alpha('#fde725ff',0.3)),
#         # Numbers
#         fontface = "bold",
#         fontfamily = "sans",
#         # Set names
#         cex = 0.5,
#         cat.cex = 0.3,
#         cat.fontface = "bold",
#         cat.default.pos = "outer",
#         cat.pos = c(-27, 27, 135),
#         cat.dist = c(0.055, 0.055, 0.085),
#         cat.fontfamily = "sans",
#         rotation = 1
# )

knitr::include_graphics("figures/venn_diagramm_fractures.png")

# final cleaning
# file.remove(list.files(path = "./figures", pattern = "\\.log$", full.names = TRUE))

```

#### iii) Cardiovascular diseases

<!-- - The variable `PWV1`, newly included in the dataset, represents an algorithmic estimation of pulse wave velocity. It is a continuous measure positively correlated with arterial stiffness. -->

-   The variables `CVALL_POST` and `CVMACE_POST` correspond to the occurrence of any cardiovascular event and **major adverse cardiovascular events** (MACE), including myocardial infarction, stroke, or cardiovascular death events, is the cognate subset of the most life-threatening heart conditions. [An extended follow-up dataset is available for `CVALL_POST`; however, it is not currently accessible.]{fg="red"}

-   The variable `MCASGLOBAL_PRE_M` captures ischemic heart disease events, at baseline.

-   The variable `AVCGLOBAL_PRE_M` represents the history of stroke events.

Contingency tables of both pre-, and post-, cardiovascular events are reported in @tbl-cardiovascular-diseases.

```{r}
#| label: tbl-cardiovascular-diseases
#| tbl-cap: "Cardiovascular disease events"
#| tbl-subcap:
#|   - "CVALL_POST vs CVMACE_POST"
#|   - "MCASGLOBAL_PRE_M vs AVCGLOBAL_PRE_M"
#| layout-ncol: 2

flextable::proc_freq(cartagene_phenodata, 
                     "CVALL_POST", "CVMACE_POST")
flextable::proc_freq(cartagene_phenodata, 
                     "MCASGLOBAL_PRE_M", "AVCGLOBAL_PRE_M")
```

Finally, we save the updated phenotype dataset as a Tab-separated file, using `readr::write_tsv`, see @lst-cartagene-plink2-prep for details:

```{r}
#| label: save-global-phenotype
#| lst-label: lst-cartagene-plink2-prep
#| lst-cap:  Turns out that newest versions of `plink2` impose explicit encoding of categorical variables, denoting as 1 'controls', and 2 as 'case'. 

# define Boolean categorical variables
categorical_variables <- cartagene_response_variables$Features[cartagene_response_variables$Type=="Boolean"]

cartagene_phenodata <- cartagene_phenodata |>
  dplyr::mutate(dplyr::across(dplyr::all_of(categorical_variables),
                              \(x) dplyr::if_else(x == 0L, 1L, 2L, missing=NA)))

readr::write_tsv(cartagene_phenodata, 
                 file="data/phenotypes/merge_phenos_PCs.txt")

# cartagene_phenodata <- readr::read_tsv("data/phenotypes/merge_phenos_PCs.txt")
```

## Step 2: Variants extraction {#sec-variant-extraction}

All the curated `VCF` files have been downloaded and processed by [Email Cartagene](mailto:access@cartagene.qc.ca). Preprocessing details are reported [here](https://cartagene.qc.ca/files/documents/other/Info_GeneticData3juillet2023.pdf)[^6]

[^6]: Use of dnaseq `Genpipes` pipelines, run on Compute Canada Clusters, version 3.1.5.

### a) HDAC-family {#sec-HDAC9}

#### i) Retrieve HDAC positions

-   **Objective**: extract the annotated VCF, for the 6 HDAC genes identified of interest, on chromosome 7 (see @sec-HDAC9 for Bash commands).

-   **Methods**:

1.  We retrieve positions of the `HDAC` genes (HDAC 4, 5, 6, 7, 9 and 10) on the latest `Hg38` Genome Build with script @lst-hdac-positions. HDAC positions are reported in @tbl-hdac-positions[^7]
2.  When several start and end positions were reported for the same gene, we consider the overall `min` starting position, and the `max` end position, respectively.

[^7]: Avoid using `BiomarT` package a tall costs: it's not anymore maintained, and leveraged out-of-date `Hg37` Human Genome Build. Report to [Genome Builds Versions](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_genome-builds-matter-avoid-costly-mistakes-activity-7293270985930141698-ZFkH) for details.

```{r}
#| label: build human database reference
#| eval: false

txdb <- GenomicFeatures::makeTxDbFromUCSC(genome = "hg38", tablename = "refGene")
AnnotationDbi::saveDb(txdb, "./data/genome_builds/human_gencode_v42.sqlite")
```

```{r}
#| label: tbl-hdac-positions
#| lst-label: lst-hdac-positions
#| lst-cap:  Use `AnnotationDBI` to fetch and retrieve automatically start and end positions of the HDAC family. 
#| tbl-cap: "HDAC Gene Positions"
#| cache: true

hdac_genes <- c("HDAC4", "HDAC5", "HDAC6", "HDAC7", "HDAC9", "HDAC10")

# Convert HGNC symbols to Entrez IDs with org.Hs.eg.db
# 1-1 mapping, great!!
hdac_entrez_ids <- AnnotationDbi::mapIds(org.Hs.eg.db, keys = hdac_genes,
                     column = "ENTREZID", keytype = "SYMBOL", multiVals = "first")
hdac_entrez_ids <- tibble::tibble(
  HGNC_SYMBOL = names(hdac_entrez_ids),
  GENEID = hdac_entrez_ids)

# retrieve start and end positions of hdac chromosoms
txdb <- AnnotationDbi::loadDb("data/genome_builds/human_gencode_v42.sqlite")
hdac_coords <- AnnotationDbi::select(txdb,
                     keys = hdac_entrez_ids$GENEID,
                     columns = c("TXCHROM", "TXSTART", "TXEND"),
                     keytype = "GENEID")

# take the min and max positions for all chromosomes
hdac_coords <- hdac_coords |>
  dplyr::group_by(GENEID, TXCHROM) |>
  dplyr::summarise(TXSTART=min(TXSTART), TXEND=max(TXEND)) |>
  dplyr::ungroup() |>
  dplyr::inner_join(hdac_entrez_ids, by = "GENEID") |>
  dplyr::arrange(GENEID) |>
  relocate(HGNC_SYMBOL)
readr::write_csv(hdac_coords,
                 "data/gene_positions/hdac_coords.csv")

# hdac_coords <- readr::read_csv("data/gene_positions/hdac_coords.csv", 
#                               show_col_types = FALSE)
flextable(hdac_coords) |> 
  bold(bold = TRUE, part = "header")
```

-   [**Remark**: Instead of fetching start and end locations automatically, add an `INFO/GENE` field directly within the `VCF` files.]{fg="red"}

#### ii) Extract SNPs corresponding to provided HDAC gene

-   **Inputs**:
    -   Cleaned **VCF** file of the chromosome 7 (where `HDAC-9` is present)[^8].
-   **Outputs**:
    -   `chr7.HDAC9.vcf`: VCF file, restrained to HDAC9 region with affiliated SNPs.
-   **Objective**: Extract annotated SNPs within `HDAC9` boundaries, as defined by the `hg38` reference genome. **Details**:
    -   Discard variants with **Minor Allele Frequency (MAF)** $< 0.01$ (in other words, the SNP must be present in at least $1\%$ of the samples).
    -   SNPs were extracted using `bcftools view`, with shell script [extract_variants.sh](./shell/extract_variants.sh)[^9]:

[^8]: All curated VCF files per chromosome are stored in `./data/genotypages/` (symbolly linked with `/mnt/projects_tn01/Cartagene/genotypage/imputation/imputation_merged`).

[^9]: `nohup` option combined with `&` character enables running variants extraction in the background, even with terminal disrupting and closing.

``` bash
nohup ./shell/extract_variants.sh > shell/extract_variants.log 2>&1 &
```

-   **Remark:** `--maf` guarantees removal of highly recessive SNPs, present in less than $1\%$ of cases with respect to the dominant form: infrequent SNPs are indeed associated with lower statistical power.

```{r}
#| lst-label: lst-vcf-header
#| lst-cap: Header lines, marked by `##` symbol, on top of VCF files. This metadata can prove useful to know exactly how SNPs were extracted in a given region.
#| label: vcf-header
#| echo: false
#| eval: false


chromosome_vcf6_file <- "../genotypage/imputation/imputation_merged/chr6.merged.clean.noMono.vcf.gz"
chromosome_vcf6_content <- readr::read_lines(chromosome_vcf6_file, n_max = 30)

# show metadata, with tools and information about VCF structure
cat(chromosome_vcf6_content |> 
      stringr::str_subset("^##"), sep = "\n")
```

<!-- #### b) HLA-family {#sec-HLA} -->

<!-- HLA genes are usually classified into **Class I (HLA-A, HLA-B, HLA-C)** and **Class II (HLA-DRB1, HLA-DQA1, HLA-DQB1)**, amounting in total to six main genes within the human organism. -->

<!-- To retrieve their positions on the latest `Hg38` Genome Build in an automated fashion, see @lst-hla-positions. HLA positions are reported in @tbl-hla-positions. [Do not use the `BiomarT` package: it's not anymore maintained, and can only be used with out-of-date `Hg37` Human Genome Build. Report to [Genome Builds Versions](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_genome-builds-matter-avoid-costly-mistakes-activity-7293270985930141698-ZFkH) for details. Besides, note that when several start and end positions were assigned to the same gene, we consider each time the minimal starting position, and the maximal end position.]{fg="red"} -->

<!-- ```{r} -->

<!-- #| label: tbl-hla-positions -->

<!-- #| lst-label: lst-hla-positions -->

<!-- #| lst-cap:  Use `AnnotationDBI` to fetch and retrieve automatically start and end positions of the HLA family.  -->

<!-- #| tbl-cap: "HLA Gene Positions" -->

<!-- # renv::install(packages = c("bioc::AnnotationDbi", "bioc::org.Hs.eg.db")) -->

<!-- # retrieve latest human genome assembly -->

<!-- # hla_genes <- c("HLA-A", "HLA-B", "HLA-C", "HLA-DQA1", "HLA-DQB1", "HLA-DRB1") -->

<!-- # # Convert HGNC symbols to Entrez IDs with org.Hs.eg.db -->

<!-- # # 1-1 mapping, great!! -->

<!-- # hla_entrez_ids <- AnnotationDbi::mapIds(org.Hs.eg.db, keys = hla_genes,  -->

<!-- #                      column = "ENTREZID", keytype = "SYMBOL", multiVals = "first") -->

<!-- # hla_entrez_ids <- tibble::tibble( -->

<!-- #   HGNC_SYMBOL = names(hla_entrez_ids), -->

<!-- #   GENEID = hla_entrez_ids) -->

<!-- #  -->

<!-- # # retrieve start and end positions of HLA chromosoms -->

<!-- # txdb <- AnnotationDbi::loadDb("data/genome_builds/txdb_human_gencode_v42.sqlite") -->

<!-- # hla_coords <- AnnotationDbi::select(txdb,  -->

<!-- #                      keys = hla_entrez_ids$GENEID,  -->

<!-- #                      columns = c("TXCHROM", "TXSTART", "TXEND"),  -->

<!-- #                      keytype = "GENEID") -->

<!-- #  -->

<!-- # # take the min and max positions for all chromosomes -->

<!-- # hla_coords <- hla_coords |>  -->

<!-- #   dplyr::filter(TXCHROM=="chr6") |>  -->

<!-- #   dplyr::group_by(GENEID, TXCHROM) |>  -->

<!-- #   dplyr::summarise(TXSTART=min(TXSTART), TXEND=max(TXEND)) |>  -->

<!-- #   dplyr::ungroup() |>  -->

<!-- #   dplyr::inner_join(hla_entrez_ids, by = "GENEID") |>  -->

<!-- #   dplyr::arrange(GENEID) |>  -->

<!-- #   relocate(HGNC_SYMBOL) -->

<!-- # readr::write_csv(hla_coords,  -->

<!-- #                  "data/gene_positions/hla_coords.csv") -->

<!-- hla_coords <- readr::read_csv("data/gene_positions/hla_coords.csv",  -->

<!--                               show_col_types = FALSE) -->

<!-- flextable(hla_coords) |>  -->

<!--   bold(bold = TRUE, part = "header") -->

<!-- ``` -->

## Step 3: GWAS Analyses

### i) GLM and GWAS {#sec-GWAS}

-   **Objective**: Use of `PLINK2 + glm` with the first 10 principal components as covariates, see @lst-glm-bash, using logistic regression for categorical variables, and `lm` for continuous outcomes.
    
-   **Input**:
    -   `VCF` file on the region/gene of interest
    -   Phenotypes, with individual patient IDs.
    -   Explanatory variable to predict, provided with `--pheno-name <response_variable>`.

::: {#lst-glm-bash}
**Template GLM instruction for GWAS studies:**

``` bash

plink2 \
  --double-id \ # <1>
  --pheno ./phenotypes/merge_phenos_PCs.txt \ # <2>
  --pheno-name OSTEOPONIA \ # <2>
  --vcf /mnt/projects_tn01/Cartagene/analyses/variants_extraction/chr7.merged.clean.noMono.extracted_variants.HDAC9.vcf dosage=HDS # <2>
  --glm hide-covar \ # <3>
  --covar /mnt/projects_tn01/Cartagene/analyses/phenotypes/merge_phenos_PCs.txt \ # <3>
  --covar-name PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10 \ # <3>
  --out HDAC9_OSTEOPONIA # <4>
```
1.  The `--double-id` option ensures independent GWAS analyses per individual, aka the *single-sample mode*, stating explicitly that the `VCF` file follows a *family-based format* (FID/IID pairs).
2.  **GWAS inputs**: We need the *phenotype information* (provided with `--pheno` and `--keep` commands), the *variable to predict* (provided with `--pheno-name`) and the *VCF file* (command `--vcf`), here using the SNPs annotations for the HDAC9 gene[^10].
3.  GWAS model options: `--glm` is the general linear model, which uses by default a *logistic regression* for categorical variables, and a *standard linear Gaussian model*, equivalent to `lm` for continuous variables[^11]. `--covar` is the covariate file, with relevant explanatory variables to integrate reported with `--covar-name` (see \@ for details). PCAs are used to describe the population structure in an unsupervised manner; and avoid and detect latent subgroups.
4.  The output GWAS folder, with `--out` command. Stored for now in `/mnt/projects_tn01/Cartagene/analyses/association`.
:::

In practice, run the [`plink2_gwas.sh`](./shell/plink2_gwas.sh) script in the background (possibility to customise gene name and feature variables to regress on):

``` bash
# Run the GWAS pipeline in the background,
# persist after logout, and log everything.

nohup ./shell/plink2_gwas.sh > shell/plink2_gwas.log 2>&1 &
```

[^10]: `dosage=HDS`, for Hard Dosage, provides the instruction describing genotype uncertainties for enhanced statistical power. It's particularly useful when working as here with **imputed genotypes**, where part of the SNPs were inferred using reference panels.

[^11]: The `hide-covar` option runs GLM, but doesn't output covariate results. Besides, note that --covar passes *quantitative covariates* (e.g., age, PCs), while `--ide-covar` passes *categorical covariates*

-   **Remark 1**: you may come up with `Error: Cannot proceed with --glm regression on phenotype 'TACAIX', since variance inflation factor for covariate 'PC2' is too high`. In this case, you may try removing completely covariates (with `--glm allow-no-covars`), or/and increase variance inflation threshold (`--vif number_vif` option)[^12]

-   [**Remark 2**: All the pre-processing operations detailed in @sec-preprocessing-redundancy are only used for the computation of the PCA components, but not subsequently used in the `glm` regression for trimming strongly correlated features.]{fg="red"}

-   [**Remark 3**: Add phenotype covariates, such as `SEX` or `AGE` in the regression framework, which can play a strong leverage on the impact of SNPs. [`GENESIS`](https://www.bioconductor.org/packages/devel/bioc/vignettes/GENESIS/inst/doc/assoc_test.html#mixed-model-association-testing) is a R package mixing environmental and genetic factors in a *fixed linear model* approach.]{fg="red"}

[^12]: **Note that a VIF factor above 10 is considered problematic as it can significantly distort regression estimates.** Discussion and visualisations on [*VIF influence*](https://www.linkedin.com/feed/update/urn:li:activity:7308463515432833026/). Briefly, VIF quantifies the degree of similarity across predictor variables, as the GLM-family usually assumes independence between explanatory variables. When VIF is significant, independence assumption is discarded, which might be explained by the fact that preprocessing in @sec-preprocessing-redundancy for removing correlated SNps and individuals are not used in subsequent GWAS differential downstream analyses.

```{r}
#| label: clean-logs
#| echo: false

genename <- "HDAC-9"
filedate <- "2025-04-07"
log_files <- list.files(file.path("tables", genename), 
                        pattern = paste0(genename, "_", ".*", "_", filedate, "\\.log$"),
                        full.names = TRUE)

if (length(log_files)>0) {
  log_contents <- vapply(
  log_files,
  FUN = function(file) paste(readLines(file, warn = FALSE), collapse = "\n"),
  FUN.VALUE = character(1)) |> 
  paste(collapse = "\n\n #################################### \n\n\n")

writeLines(log_contents, 
           con = file.path("tables", genename, paste0("GENERAL_", genename, "_", filedate, ".log")))

file.remove(log_files)

}
```

### ii) GWAS Visualisations

For each gene-phenotype pair, we generate cognate **Manhattan plots**[^13], and $p$-values distributions, using *histograms*[^14], and **QQplots**[^15]. Core function for generating these 3 GWAS visualisations is [`generate_gwas_per_phenotype`](./R/gwas_plots.R). An example for gene `HDAC-9` is provided in @fig-GWAS-HDAC9.

[^13]: More customisation available in [Manhattan plot in R: a review](https://r-graph-gallery.com/101_Manhattan_plot.html)

[^14]: [How to interpret a $p$-value histogram](https://varianceexplained.org/statistics/interpreting-pvalue-histogram/)

[^15]: [How I Make QQ Plots Using `ggplot2`?](https://danielroelfs.com/blog/how-i-make-qq-plots-using-ggplot/)

```{r}
#| label: gwas-visu-preparation

# p-value adjustment
# genename <- "HDAC-9"; pheno_label <- "OSTEOPONIA"; filedate <- "2025-04-07"
num_variants <- readr::read_csv("./tables/HDAC-9/HDAC-9_AVCGLOBAL_PRE_M_2025-04-07.AVCGLOBAL_PRE_M.glm.logistic.hybrid", show_col_types = FALSE) |> nrow()
pval_threshold <- 0.05/(num_variants*length(cartagene_response_variables$Features))
# pval_threshold <- 0.01/num_variants

gwas_plots_hdac <- lapply(cartagene_response_variables$Features, 
                          function(feature_label) generate_gwas_per_phenotype (genename = "HDAC-9",
                                                                               pheno_label = feature_label, filedate="2025-04-07", pval_threshold = 0.001)) 

gwas_plots_hdac <- gridExtra::marrangeGrob(gwas_plots_hdac, nrow=1, ncol=1)

ggsave("figures/gwas_HDAC9.pdf",
       gwas_plots_hdac, dpi = 600,
       width = 8, height = 12) 
```

::: {#fig-GWAS-HDAC9}

{{< pdf ./figures/gwas_HDAC9.pdf width=80% height=800 >}}

:::

### iii) Multiple test correction

To compute the adjusted *significance threshold*, we applied a `FWER`-like approach, conventional in the GWAS field: `{{r}} 0.05/(num_variants*length(cartagene_response_variables$Features))`, in which you divide the pre-defined threshold (standard: `0.05`) by the total number of pairwise tests carried out (number of variants times number of regression variables times). Final threshold is accordingly: `{r} 0.05/(num_variants*length(cartagene_response_variables$Features))`, as `{r} num_variants` variants were extracted, and `{r} length(cartagene_response_variables$Features)` phenotype variables were predicted.

[The FWER approach is particularly **conservative**, in other words, only the most significant differences are detected.]{fg="red"}

-   **Perspectives**:

    -   [Recent Adjusted $p$-value correction method](https://www.linkedin.com/posts/adrianolszewski_statistic-datascience-research-activity-7301605962820284417-ho3Z).
    -   [Multiple Comparisons Using R](https://www.taylorfrancis.com/books/mono/10.1201/9781420010909/multiple-comparisons-using-frank-bretz-torsten-hothorn-peter-westfall), comprehensive resource of distinct R package strategies for correcting for multiplicity of $p$-values, from @bretz2016. Report also to @nte-MTC for an overview of existing approaches.

::: {#nte-MTC .callout-note title = "Classes of Multiple Test correction approaches" collapse = "true"}

**Multiple comparison procedures** (MCPs) can be classified into the following adjustment methods:

1.  *Fixed sequential*: no adjustment as long as H0s are rejected. One non-rejection and it stops testing with all others non-rejected.

2.  *Callback*: significance level (e.g. 0.05) is split between all hypotheses (can be further weighted). With rejection of the subsequent H0s, their fractions of significance level is accumulated and passed to the next comparison.

3.  *Gatekeeper*: serial, parallel & combined trees, collecting hypotheses into families. Groups of testing can be interpreted as *gatekeepers*: only if any $H_0$ in a family is rejected that the significance is propagated to the next family. The approach is the most versatile, and can be flexibly implemented in R package [`gMCPLite`](https://merck.github.io/gMCPLite). See also [Graphical Approaches to Multiple Test Problems](https://baselbiometrics.github.io/home/docs/talks/20220329/2_Glimm_Bretz_Xi.pdf), for additional details.

:::

# Perspectives, aka post GWAS

## Explore non-coding, regulatory regions (from WES to WGS)

- Paper [Systematic differences in discovery of genetic effects on gene expression and complex traits](https://www.nature.com/articles/s41588-023-01529-1), from @mostafavi2023ng, suggest extending extraction of genomic regions of interest beyond coding sequences (report to @sec-variant-extraction for current approach). Indeed, the article demonstrates that most of identified signals originate from nearby gene regulatory sites. 

- [`EPInformer`: a scalable deep learning framework for gene expression prediction by integrating promoter-enhancer sequences with multimodal epigenomic data](https://www.biorxiv.org/content/10.1101/2024.08.01.606099v1), from @lin2024. Avalaible as a [GH package](https://github.com/pinellolab/EPInformer). Can infer and predict gene expression from promoter and enhancer sequences paired with epigenomic signals; identify the most significant enhancers; and identify *regulatory sequences* and *transcription factor binding motifs*.

## Post-GWAS: Explore other types of genetic variations

- [Genome-wide association testing beyond SNPs](https://www.nature.com/articles/s41576-024-00778-y), from @harris2025nrg

- [Benchmarking post-GWAS analysis tools in major depression: Challenges and implications](https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.1006903/full), from @perez-granado2022fg.

## Post-GWAS: Retrieve automatically variants of interest {#sec-post-GWAS}

This analysis is not formally a GWAS study, but rather a *gene-centric* study, pre-selecting genes based on prior knowledge and/or intuition.

Alternatively, we could consider a data-driven, and more agnostic approach, to refine the selection of gene candidates. Several approaches to that end have been implemented:

-   @gnanaolivu2025bb proposed in [A clinical knowledge graph-based framework to prioritize candidate genes for facilitating diagnosis of Mendelian diseases and rare genetic conditions](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-025-06096-2), the `phenotype prioritization and analysis for rare diseases, PPAR` algorithm to rank genes based on *human phenotype ontology (HPO)* terms.[^16]. In details, `PPAR` combines embeddings from the *human knowledge graph*, incorporating genes, HPO terms, and gene ontology annotations connections. For each input HPO term, a prioritized list of genes is returned based on their relevance and similarity to the HPO term.
-   @bridges2025bb developped in [Towards a standard benchmark for phenotype-driven variant and gene prioritisation algorithms: `PhEval`, Phenotypic inference Evaluation framework](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-025-06105-4) a unified benchmarking platform to clean inputs for *phenotype-driven VGPAs*. Variant and Gene Prioritisation Algorithms integrate complex and multi-modal datasets, such as ontologies and gene-to-phenotype associations, to predict the most influential and promising gene targets controlling the evolution of rare diseases. Model is available as GitHub Repo [`PhEval`](https://github.com/monarch-initiative/pheval).

[^16]: Note that relying on HPO terms, for example, `HP:0004322 – Osteopenia`, would alleviate the need for standardising phenotype labels, using universally acknowledged scientific terms instead.

## Post-GWAS: Incorporate familial pedigrees

-   [Genome-wide association study of fat content and fatty acid composition of shea tree.](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-025-11344-z), from @attikora2025bg. Under the hood, relies on the `mrMLM` R package to include both the *population structure matrix* and the *kinship matrix*. Provides a complete GWAS pipeline, including a number of meaningful illustrations.

-   [`BHCox`: Bayesian heredity-constrained Cox proportional hazards models for detecting gene-environment interactions](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-025-06077-5), from @sun2025bb. Under the hood, relies on the `brms` R package.

## Post-GWAS: Polygenic risk scores

- [Optimizing and benchmarking polygenic risk scores with GWAS summary statistics](https://doi.org/10.1186/s13059-024-03400-w), from @zhao2024gb.

-   [`XPRS`: A Tool for Interpretable and Explainable Polygenic Risk Score](10.1093/bioinformatics/btaf143), from @kim2025b, is a R package for inferring the *polygenic risk score* for assessing genetic susceptibility to diseases, providing additional interpretation and visualisation tools. PRSs are further split and classified into genes and single nucleotide polymorphism (SNP) contribution scores via *Shapley additive explanations* (SHAPs), visualised as **Manhattan plots**, **LocusZoom-like** plots and tables at the population and individual levels.

-   [`PNL`: a software to build polygenic risk scores using a Super Learner approach based on `PairNet`, a Convolutional Neural Network](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btaf071/8015614), from @chen2025b.

## Post-GWAS: Orthogonal integration

### Epigenomic

-   @zhao2025bib implements [`SC-VAR`: a computational tool for interpreting polygenic disease risks using single-cell epigenomic data](https://academic.oup.com/bib/article/26/2/bbaf123/8092303?searchresult=1&login=false), a novel computational tool available as a GitHub repo and Python application: [`SC-VAR` GitHub](https://github.com/gefeiZ/SC_VAR). `SC-VAR` uses single-cell *epigenomic* data to predict functional outcomes of the identified disease-associated *GWAS variants*, enhancing their interpretability. Under the hood, relies on [`MAGMA` linear modelling framework](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004219), developed by @leeuw2015pcb.

### eQTL (quantitative expression)

- [`FIVEx`](https://github.com/statgen/fivex) is a web-based interface to explore `CarTaGene` to visualize and query eQTL (quantitative trait loci) data in various ways. **Would need a stringent maintainance!!**

# Appendix {.appendix}

## Scalability with `Nextflow` {#sec-DSL-Nextflow}

Lots of intermediate files are not required for downstream analyses, hence it would be relevant to rely on an existing *Nextflow* or *Snakemake* DSL workflows:

-   [`nf-GWAS` 'Nextflow pipeline'](https://genepi.github.io/nf-gwas/), from @schonherr2024, is actively maintained by Curie Bioinformatics Team, and includes the latest `plink2` facilities.

-   [`PopGLen` 'Snakemake' pipeline](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btaf105/8069456), from @nolen2025b. [Not for running GWAS analyses, but rather for evaluating the quality and impact of preprocessing and quality mapping at the population-level genome starting from `FASTQ` files.]{fg="red"}

## Tools Installation {#sec-tools-install}

 {{< include tools_installation.qmd >}}

## Clean Git History

``` bash

python3 bin/git-filter-repo --path data/variants/chr6.HLA-complex.vcf --invert-paths

```

## Bibliographic References {.appendix}

::: {#refs}
:::
