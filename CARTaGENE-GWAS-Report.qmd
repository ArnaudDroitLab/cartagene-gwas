---
title: "GWAS Analyses utilising the CARTaGENE database"
author: "Bastien CHASSAGNOL and Marie-Pier and Clément and Fabrice"
date: last-modified	
number-sections: true
toc: true
toc-depth: 4
lang: en-GB
bibliography: Cartagene.bib
link-citations: true
highlight-style: github
filters:
  - highlight-text
# code options
execute:
  message: false
  warning: false
  error: false
  eval: true
  echo: true
# Table options
tbl-cap-location: bottom
format: 
  html:
    embed-resources: true
    toc-location: left
    toc-expand: 3
    theme:
      light: cosmo
      dark: cosmo
    sidebar: true
    lightbox: true
    comments: 
      hypothesis: true
    # code options
    code-fold: show
    code-link: true
    code-annotations: hover
    page-layout: full
    collapse: true
  docx:
    toc-title: Contents
editor: source
---

# Reproduce the pipeline and questions to address

## Two-step pipeline 

Assuming you validate all *pre-processing steps* reported in @sec-white-selection and @sec-preprocessing-redundancy, to adjust the pipeline to any target gene of interest, follow this two-step protocol:

  i.  Retrieve all known variants associated with your genes of interest in @sec-variant-extraction, and generate the corresponding VCF file.
  ii. Change the response variable to predict in @lst-glm-bash (with option `--glm`, the model applied should adjust automatically whether the outcome is binary or continuous). **If the outcome is ordinal, or multi-class, then further statistical considerations must be accounted**.
  
## Scalability with `Nextflow` {#sec-DSL-Nextflow}

Lots of intermediate files are not required for downstream analyses, hence it would be relevant to rely on an existing *Nextflow* or *Snakemake* DSL workflows:

-   [`nf-GWAS` 'Nextflow pipeline'](https://genepi.github.io/nf-gwas/), from from @schonherr2024, is actively maintained by Curie Bioinformatics Team, and includes the latest `plink2` facilities.


-   [`PopGLen` 'Snakemake' pipeline](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btaf105/8069456), from @nolen2025b. [Not for running GWAS analyses, but rather for evaluating the quality and impact of preprocessing and quality mapping at the population-level genome starting from `FASTQ` files.]{fg="red"}


## Reproducibility

Two CLI tools are needed for reproducing the analyses and/or delivering new outcomes:

-   Core tool: `plink2` for running most of the SNPs analyses, currently in two locations: `/home/galgen01/programs/plink` and `/usr/bin/plink2`.
-   `bcftools` for extracting variants of interest in a given genomic region: `/home/galgen01/programs/bcftools/bin/bcftools` and `/mnt/software/is1/commonSoftware/bcftools/bcftools-1.3.1/bin/bcftools`.


Besides, use Git + GitLab to version the code, work in a R project to compartmentalise datasets and scripts and `renv()` to create a reproducible R snapshot environment.

## Important Questions to Address!! {#sec-questions}

-   [Check Human reference genome assembly, see @sec-HDAC9. Indeed, among the five genome arrays assembled in @sec-white-selection, at least one turns out to have been mapped against `GRCh37 (hg19)` older reference instead of recent `hg38 (GRCh38, 2013)`.]{fg="red"}
-   [Why starting from BAM/BED/BIM pre-processed files in @sec-white-selection, instead of `FASTQ` files available in `/mnt/projects_tn01/Cartagene/data/merged`? For DSL workflows [@sec-DSL-Nextflow], you may directly provide raw sequences in the pipeline. Related question: we use the `BAM/BED/BIM` files stored in folder `/mnt/projects_tn01/Cartagene/analyses/QC` for PCA computation in @sec-preprocessing-redundancy, but VCF files stored in `/mnt/projects_tn01/Cartagene/analyses/variants_extraction` for running the GWAS analyses, for which reason?]{fg="red"}
-   [Why use a biased *gene-centric approach*, instead of true GWAS (further discussed in @sec-post-GWAS)?]{fg="red"} 
- **(Optional:1)** use approved **human phenotype ontologies (HPO)** in @sec-phenotype-extraction to definite phenotype features of interest.
- (**Optional 2**): consider using BCF files instead of VCF files, the former being binary and compressed versions of the latter

# Analyses

- [File Naming Convention, waiting for True Database configuration, is detailed in the two following LinkedIn posts: [here](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_the-hardest-part-of-bioinformatics-naming-activity-7295090463894589441-Ra-j), and [there](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_stop-losing-track-of-your-analysis-files-activity-7311382817110708225-Kl3A). Add date, file type and extension, along with model organism or study name + details General File Organisation.]{fg="red"}

- [Consider adding *symbolic links* to encapsulate the whole project, see [`ln -s` Post](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_the-most-underrated-unix-command-for-bioinformatics-activity-7287842704753848321-qhe) for details.]{fg="red}

- Bash configuration

```{bash}
export PATH=./bin/bcftools/bin:$PATH
export PATH=./bin/plink2:$PATH

# create symbolic links to organise everythin within the same folder
# warning: if original content is deleted, everything broke!!!!!!
ln -s ./data/genotypages/ /mnt/projects_tn01/Cartagene/genotypage/imputation/imputation_merged
```

- R configuration:

```{r}
#| label: setup
#| include: false
# data wrangling and visualisations
library(haven)
library(flextable)
library(dplyr)
library(ggplot2)
library(VennDiagram)
# Required for code linking
library(downlit)
library(xml2)

# Retrieve gene positions
library(org.Hs.eg.db)  # Provides gene symbol to Entrez ID mapping

# for generating nice visualisations
source("R/gwas_plots.R")
```

## Step 1: Merge genotype arrays {#sec-white-selection}

Genotypes were split into five different genotyping arrays. Arrays were merged together using Bash script [merge_datasets.sh](/mnt/projects_tn01/Cartagene/analyses/QC/merge_datasets.sh), and command `--bmerge` of tool `plink2`. This command only keeps matching **SNPs** and **alleles** in the 5 datasets.

The resulting `BAM` files (sequence alignments), `BED` files (genomic regions of interest, for viewers), `BIM` (SNP information) and `FAM` (phenotype data, such as individuals and pedigree), are listed in folder `/mnt/projects_tn01/Cartagene/analyses/QC`, with prefix `merge_5_*` (for 5 genotypes concatenated).

## Step 2: Preprocessing for Removing strongly correlated gene variants and familial samples {#sec-preprocessing-redundancy}

**Remark**: the `eur_only` gathers Eurasian, white phenotypes, hence the name of the folder.

### i) Trim missing SNPs {#sec-missing-SNPs}

-   **Inputs**:

    -   **Phenotype IDs of white individuals**: `eur_only/cartagene_self_reported_EUR.plink_format.txt`
    -   **BIM/BED/FAM/hh** folder generated with `--make-bed` command: `analyses/QC/merge_5_datasets`. [It seems that among the 5 genotype arrays concatenated, at least one genotype, namely `gsa.17k.final.hg19.bim,` has been mapped thanks to `GRCh37 (hg19)` reference instead of more recent `hg38 (GRCh38, 2013)` version.]{fg="red"}]

-   **Objective**: Keep white individuals and remove SNPs variants with genotyping rate $<0.95$. The **genotyping rate** measures the proportion of successfully genotyped markers and indicates how complete the genotype data is. Removing SNPs with low Genotyping Rate increases Missing Data imputation performance, and overall maintains higher statistical power.

-   [**Remark**: Also advised to exclude samples with low Genotyping Rate, below 0.98. Not done in this study. To be checked]{fg="red"}

-   **Bash command**:

``` bash
/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/merge_5_datasets \
  --geno 0.05 \
  --keep /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/cartagene_self_reported_EUR.plink_format.txt \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095
```

### ii) Hardy-Weinberg Disequilibrium {#sec-HWD}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files generated from previous SNP missing removal, see @sec-missing-SNPs.

-   **Objective**: Remove SNPs variants with **Hardy-Weinberg disequilibrium** $< 1 \times 10^{-6}$. Indeed, if a SNP is in HWD, it might reflect hidden sub-population structure or poor-quality rather than true genetic associations.

-   [**Remark**: If a SNP is under strong **natural selection**, such as SNPs involved in the HLA genes, they are likely to deviate from HWE due to balancing selection. If the SNP is biologically important, don't exclude it blindly!! Alternative: compute the SNP score.]{fg="red"}

-   **Bash command**:

``` bash
/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095 \
  --hwe 1e-6 \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06
```

### iii) Linkage Disequilibrium {#sec-LD}

#### Local LD analysis {#sec-local-LD}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `/merge_5_datasets.eur_only.geno095.hwe1e0` generated from previous HWD SNP trimming, see @sec-HWD.

-   **Objective**: Remove SNPs variants associated with strong **linkage disequilibrium**. In an ideal population under random mating, allele combinations should be independent ($r^2=0$). However, due to factors such as genetic drift, or physical proximity, certain alleles tend to be inherited together more often than expected. In GWAS, LD pruning avoids **Overfitting** (SNPs in high LD carry redundant information), avoiding spurious inflation of GWAS association signals.

-   [**Remark**: Report the Number of SNPs Before and After Pruning, usually more sringent $R^2$ is considered.]{fg="red"}

-   **Tool**: `plink --indep-pairwise 50 5 0.5` will discard SNPs with a correlation coefficient above $0.5$ (given that a $R^2$ score of 1 indicates a perfect correlation), on a *rolling window* of $50$ SNPs (focus on local zones) and step $5$ by $5$.

-   **Bash command**:

``` bash
# identify SNPs with low LD
/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06 \
  --indep-pairwise 50 5 0.5 \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps
  
# save pruned SNPs associated with low LD
/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06 \
  --extract /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.prune.in \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile
```

#### Global LD analysis from prior expert knowledge {#sec-global-LD}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile` generated from previous local LD trimming, see @sec-local-LD.

-   **Objective**: Remove SNPs variants associated with strong **linkage disequilibrium** genomic regions from prior expert knowledge. In details, list of regions to be excluded is reported [here](http://dougspeed.com/wp-content/uploads/highld.txt), as generated by the Abecasis Group in 2023[^1]

-   **Bash command**:

[^1]: Notably includes MHC, lactase region, and known inversions `8p23` and `17q21.31`. **Haplotypes**?

``` bash

/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile \
  --exclude range /mnt/projects_tn01/Cartagene/analyses/QC/high_ld_regions.plink_format.txt \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded
```

### iv) Exclude affiliated individuals based on high IBD scores

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile` generated from previous local LD trimming, see @sec-local-LD.

-   **Objective**: Exclude related individuals computing **identity by descent** (IBD), a genetic metric of the relatedness between two individuals, and exclude one individual by pairs of individuals with a score above $0.2$. Indeed, strongly associated patients increase bias in GWAS, raising a stronger score than expected in the general population, and prevents quality controls to detect duplicates or sample mix-ups.

-   **Details**: A two-step, more stringent IBD filtering strategy has been chosen, eliminating *first* the most problematic individuals, and the *second* ensuring the remaining related individuals are properly filtered (for each correlated pair, prune randomly one of them). The Bash instructions are reported [here](/mnt/projects_tn01/Cartagene/analyses/QC/eur_only/related_ids/Readme.txt).

-   [**Remarks**: Choice of a heuristic threshold of $n=68$ affiliated patients removal to be further discussed. Current score of `PI_HAT` score of $0.2$ is surprising, as common thresholds are either $0.25$ for discarding grandparent-grandchild, or $0.125$ or lower for only keeping the most distant relatives. Starting from `PLINK 2.0`, the recommended approach is now utilizing the `--king-cutoff` command, over the older `--rel-cutoff` and `--genome --min` + aggregates all subsequent steps simultaneously, especially in heterogeneous populations]{fg="red"}[^2].

-   **Bash command**:

[^2]: [`--king-cutoff 0.0884` corresponds to `PI_HAT = 0.125`, see details [here](http://biostars.org/p/434832/#434898)]{fg="red"}

``` bash

# Calculate IBD, select individuals with IBD above 0.2 for further pruning

/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile \
  --genome \
  --memory 12006 \
  --min 0.2 \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.IBD


# Remove related individuals, in a two-stage process

/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_1_of_2 \
  --remove /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/related_ids/IBD.genome.iids.merged.sorted.count.reverse.ids_related_to_2_individuals.ids_only.FID_IID_format


/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_1_of_2 \
  --make-bed \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_2_of_2 \
  --remove /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/related_ids/IBD.genome2.related_individuals_to_remove.txt 
```

## Step 3: PCA {#sec-PCA}

-   **Inputs**:

    -   **BIM/BED/FAM/hh (Homozygous-Haplotype)** files `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded` resulting from the pre-processing operations reported in @sec-preprocessing-redundancy (LD, HWD and missing SNPs trimming)
    -   **phenotype IDs** listed in `merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_2_of_2`, where all affiliated patients have been removed.

-   **Objective**: Compute PCA and keep the 10 first principal components.

-   [**Remark**: use visualizations, such as *scree plots*, elbow point or/and *Tracy-Widom Test* to select the final number of PCs, instead of hard threshold, and on the other hand, scatter plots to identify latent structures. See details [here](https://www.linkedin.com/posts/joachim-schork_statistics-research-visualanalytics-activity-7251577056570785795-b6Rk) and [there](https://www.linkedin.com/feed/update/urn:li:activity:730919293617122508)]{fg="red"}.

-   **Bash command**:

``` bash

/home/galgen01/programs/plink \
  --bfile /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded \ 
  --keep merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps_bfile.rem_rel_ind_round_2_of_2.fam \
  --memory 12006 \
  --out /mnt/projects_tn01/Cartagene/analyses/QC/eur_only/merge_5_datasets.eur_only.geno095.hwe1e06.pruned_snps.high_LD_excluded.unrelated_ind.PCA \
  --pca 10
```

## Step 4: Phenotype feature extraction {#sec-phenotype-extraction}

Original phenotype annotations are available in folder `/mnt/projects_tn01/Cartagene/analyses/phenotypes`.
Variables of interest are listed in:

```{r}
#| label: tbl-cartage-pheno-features
#| tbl-cap: Features of interest, and response variables, to predict in `CarTaGene`.

cartagene_response_variables <- readxl::read_excel("data/phenotypes/cartagene_response_variables.xlsx")

flextable(cartagene_response_variables) |> 
  bold(part="header")

```


### i) Merge genotype IDs, phenotypes and PCAs values

General phenotype features are provided in

```{r}
#| label: tbl-cartage-pheno
#| tbl-cap: Read SAS table describing CarTaGene phenotypes.

cartagene_phenodata <- haven::read_sas("data/phenotypes/cart_mars2025.sas7bdat")
flextable(head(cartagene_phenodata)) |> 
  bold(bold = TRUE, part = "header")

```

Number of patients overall is `{r} nrow(cartagene_phenodata)`, and number of phenotype variables is `{r} ncol(cartagene_phenodata)`.

We then need to map each individual patient ID (`IID`) with its corresponding genotype array [@tbl-cartage-genotypes-IDs], as done in @lst-join-phenotype-genotype.

```{r}
#| label: tbl-cartage-genotypes-IDs
#| tbl-cap: Read SAS table describing CarTaGene phenotypes.
cartagene_genotypes_ID <- readr::read_csv2("./data/phenotypes/cartagene_genotype_IDs.csv",
                                           show_col_types = FALSE, 
                                           col_types = c("d", "c","c")) |> 
  dplyr::rename(PROJECT_CODE = "project_code", geno_id = "file_111", batch="batch")
  

flextable(head(cartagene_genotypes_ID)) |> 
  bold(bold = TRUE, part = "header")
```

[Avoid using French CSV settings, switch to universal convention, where delimiter is a comma: `,`]{fg="red"}

```{r}
#| label: join-phenotype-genotype
#| lst-label: lst-join-phenotype-genotype
#| lst-cap: Inner join between phenotypes IDs and genotypes, while constraing the remaining individuals to belong to white ethnicity. 
cartagene_phenodata <- cartagene_phenodata |> 
  dplyr::inner_join(cartagene_genotypes_ID, by="PROJECT_CODE") |> 
  dplyr::filter(ETHNICITY6M=="Blanc")
```

The resulting phenotype table, after joining with genotypes IDs and restraining to Eurasian phenotypes, has been restrained to `{r} nrow(cartagene_phenodata)` individuals.

### ii) PCA merging

PCA vectors computed in @sec-PCA are subsequently merged with phenotype data in @lst-join-phenotype-PCA.

```{r}
#| label: join-phenotype-PCA
#| lst-label: lst-join-phenotype-PCA
#| lst-cap: Inner join between phenotypes IDs and first 10 PCA eigen vectors.

PCs <- readr::read_delim("data/PCAs/PCA_eigenvec",
                        col_names = c("FID", "geno_id", "PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9", "PC10"), 
                        delim = " ",
                        show_col_types = FALSE)
cartagene_phenodata <- PCs |> 
  dplyr::inner_join(cartagene_phenodata, by="geno_id") |> 
  dplyr::rename(IID = geno_id)

```

### iii) Bone damage: osteoporosis and fractures

- The response variables `OSTEOPONIA` and `OSTEOPOROSIS` have been computed following these rules [@lst-OSTEOPONIA-computation], with the resulting contingency tables reported in @tbl-OSTEOPONIA-computation.


  1.  Exclude patients with `osteosecondaire==1`.^[Currently done by assigning all corresponding observations a missing value. To be replaced by extracting patients with `osteosecondaire` and subsequently use `--remove` option when running `glm + plink2`.]{fg="red"}
  2.  `OSTEOPONIA` is `case: 1` if `DMOTSCORE_mod <= -1.5` and `control` elsewhere^[`DMOTSCORE_mod` represents the measurement of bone mineral density at the calcaneus. While no universally accepted threshold exists for the diagnosis of osteopenia or osteoporosis, we adopt the conventional cut-off of `-1.5`.].
  3.  `OSTEOPOROSIS` is `case: 1` if `DMOTSCORE_mod < -2.5` and `control` elsewhere. [`DMOTSCORE_mod` has been likely Gaussian standardised + lacks of evidence supporting clear multi-modal distributions.]{fg="red"}.

```{r}
#| label: tbl-OSTEOPONIA-computation
#| lst-label: lst-OSTEOPONIA-computation
#| lst-cap: Generate scores of interest.
#| tbl-cap: `OSTEOPOROSIS` is considered as more severe than `OSTEOPONIA`, hence the striclty lower number of individuals affected by the disease. 
# NA values treated in the LHS conditions as FALSE, assigning them the .default value. 
cartagene_phenodata <- cartagene_phenodata |> 
  dplyr::mutate(DMOTSCORE_mod = dplyr::if_else(OSTEOSECONDAIRE ==1, NA, DMOTSCORE), 
                OSTEOPONIA = dplyr::case_when(DMOTSCORE_mod <= -1.5 ~ 1, 
                                              DMOTSCORE_mod > -1.5 ~ 0, 
                                              .default = NA),
                OSTEOPOROSIS = dplyr::case_when(DMOTSCORE_mod < -2.5 ~ 1, 
                                              DMOTSCORE_mod >= -2.5 ~ 0, 
                                              .default = NA)) 
  

flextable::proc_freq(cartagene_phenodata, 
                     "OSTEOPONIA", "OSTEOPOROSIS")

```


- **Fracture events**:

  1. We merge `FXALL_PRE5` (any fracture occurring within the five years prior to recruitment) with `FXALL_POST` (all fractures occurring during follow-up) into variable `FXALL`, being positive if an event fracture occured prior or posterior. [Which variable between `FXALL_PRE5` and `FXALL_PRE1` should be considered? Variables differ by `{r} length(which(cartagene_phenodata$FXALL_PRE5!=cartagene_phenodata$FXALL_PRE1)/nrow(cartagene_phenodata))` $\%$ overall!!]{fg="red"}
  2. `FXMOF_POST` (only osteoporotic fractures during follow-up)^[For both `FXMOF_POST` and `FXALL_POST`, the dates of occurrence and censoring dates are available in `FXALL_POSTDATE` and `FXMOF_POSTDATE` respectively.] [In contrast with labelling, osteoporotic events, as stored in `FXMOF_POST`, contain both pre-, and posterior fracture events, report to Venn Diagram @fig-venn-osteo for details. Venn Diagrams could be improved with [`ggVennDiagram` package](https://gaospecial.github.io/ggVennDiagram/articles/using-ggVennDiagram.html)]{fg="red"} 
  
```{r}
#| label: tbl-fracture-computation
#| lst-label: lst-fracture-computation
#| lst-cap: Aggregate all fracture events into a single one (pre- and post- operation).
#| tbl-cap: Description of fracture events.
#| tbl-subcap:
#|   - "All fractures vs osteporotic fractures"
#|   - "Pre vs post-fractures."
#| layout-ncol: 2

cartagene_phenodata <- cartagene_phenodata |> 
  mutate(FXALL = dplyr::if_else(FXALL_POST == 1L | FXALL_PRE1 ==1L, 1L, 0L))

flextable::proc_freq(cartagene_phenodata, 
                     "FXALL", "FXMOF_POST")

flextable::proc_freq(cartagene_phenodata, 
                     "FXALL_POST", "FXALL_PRE1")
```

  
```{r}
#| label: fig-venn-osteo
#| fig-cap: "Venn Diagrams of fracture events."

labels_OSTEO <- list(`post fracture` = cartagene_phenodata$IID[cartagene_phenodata$FXALL_POST==1], 
                          `pre facture` = cartagene_phenodata$IID[cartagene_phenodata$FXALL_PRE1==1], 
                          `osteoporotic fracture` = cartagene_phenodata$IID[cartagene_phenodata$FXMOF_POST==1])
# Chart
VennDiagram::venn.diagram(
        x = labels_OSTEO,
        filename = 'figures/venn_diagramm_fractures.png',
        output=TRUE,
        # Output features
        imagetype="png" ,
        height = 480 , 
        width = 480 , 
        resolution = 300,
        # Circles
        lwd = 2,
        lty = 'blank',
        col=c("#440154ff", '#21908dff', '#fde725ff'),
        fill = c(scales::alpha("#440154ff",0.3), scales::alpha('#21908dff',0.3), scales::alpha('#fde725ff',0.3)),
        # Numbers
        fontface = "bold",
        fontfamily = "sans",
        # Set names
        cex = 0.5,
        cat.cex = 0.4,
        cat.fontface = "bold",
        cat.default.pos = "outer",
        cat.pos = c(-27, 27, 135),
        cat.dist = c(0.055, 0.055, 0.085),
        cat.fontfamily = "sans",
        rotation = 1
)

knitr::include_graphics("figures/venn_diagramm_fractures.png")

# final cleaning
file.remove(list.files(path = "./figures", pattern = "\\.log$", full.names = TRUE))

```


### iv) Cardiovascular diseases

<!-- - The variable `PWV1`, newly included in the dataset, represents an algorithmic estimation of pulse wave velocity. It is a continuous measure positively correlated with arterial stiffness. -->

- The variables `CVALL_POST` and `CVMACE_POST` correspond to the occurrence of any cardiovascular event and **major adverse cardiovascular events** (MACE), including myocardial infarction, stroke, or cardiovascular death events, is the cognate subset of the most life-threatening heart conditions. [An extended follow-up dataset is available for `CVALL_POST`; however, it is not currently accessible.]{fg="red"}

- The variable `MCASGLOBAL_PRE_M` captures ischemic heart disease events, at baseline.

- The variable `AVCGLOBAL_PRE_M` represents the history of stroke events.

Contingency tables of both pre-, and post-, cardiovascular events are reported in @tbl-cardiovascular-diseases. 


```{r}
#| label: tbl-cardiovascular-diseases
#| tbl-cap: "Cardiovascular disease events"
#| tbl-subcap:
#|   - "CVALL_POST vs CVMACE_POST"
#|   - "MCASGLOBAL_PRE_M vs AVCGLOBAL_PRE_M"
#| layout-ncol: 2

flextable::proc_freq(cartagene_phenodata, 
                     "CVALL_POST", "CVMACE_POST")
flextable::proc_freq(cartagene_phenodata, 
                     "MCASGLOBAL_PRE_M", "AVCGLOBAL_PRE_M")
```

Finally, you can save the global phenotype dataset, merging PCs, phenotype and genotype IDs along with metric computation, using `readr::write_csv`:

```{r}
#| label: save-global-phenotype
#| lst-label: lst-cartagene-plink2-prep
#| lst-cap:  Turns out that newest versions of `plink2` impose explicit encoding of categorical variables, denoting as 1 'controls', and 2 as 'case'. 

# define Boolean categorical variables
categorical_variables <- cartagene_response_variables$Features[cartagene_response_variables$Type=="Boolean"]

cartagene_phenodata <- cartagene_phenodata |>
  dplyr::mutate(dplyr::across(dplyr::all_of(categorical_variables),
                              \(x) dplyr::if_else(x == 0L, 1L, 2L, missing=NA)))

readr::write_tsv(cartagene_phenodata, 
                 file="data/phenotypes/merge_phenos_PCs.txt")

# cartagene_phenodata <- readr::read_tsv("data/phenotypes/merge_phenos_PCs.txt")
```

## Step 5: Variants extraction {#sec-variant-extraction}

All the curated VCF files have been downloaded and processed by an external third-party supplier, namely [Email Cartagene](mailto:access@cartagene.qc.ca). Details on how to access and process the database can be found [here](https://cartagene.qc.ca/files/documents/other/Info_GeneticData3juillet2023.pdf)[^3]

[^3]: Use of dnaseq `Genpipes` pipelines, run on Compute Canada Clusters, version 3.1.5.

#### a) HDAC-family {#sec-HDAC9}


##### i) Retrieve HDAC positions

-   **Objective**: extract the annotated VCF, for the 6 HDAC genes identified of interest, on chromosome 7 (see @sec-HDAC9 for Bash commands). 

-   **Methods**: To retrieve positions of the `HDAC` genes (HDAC 4, 5, 6, 7, 9 and 10) on the latest `Hg38` Genome Build in an automated fashion, report to @lst-hdac-positions. HLA positions are reported in @tbl-hdac-positions. [Do not use the `BiomarT` package: it's not anymore maintained, and can only be used with out-of-date `Hg37` Human Genome Build. Report to [Genome Builds Versions](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_genome-builds-matter-avoid-costly-mistakes-activity-7293270985930141698-ZFkH) for details. Besides, note that when several start and end positions were assigned to the same gene, we consider each time the minimal starting position, and the maximal end position.]{fg="red"}

```{r}
#| label: build human database reference
#| eval: false

txdb <- GenomicFeatures::makeTxDbFromUCSC(genome = "hg38", tablename = "refGene")
AnnotationDbi::saveDb(txdb, "./data/genome_builds/human_gencode_v42.sqlite")
```


```{r}
#| label: tbl-hdac-positions
#| lst-label: lst-hdac-positions
#| lst-cap:  Use `AnnotationDBI` to fetch and retrieve automatically start and end positions of the HDAC family. 
#| tbl-cap: "HDAC Gene Positions"
#| cache: true

hdac_genes <- c("HDAC4", "HDAC5", "HDAC6", "HDAC7", "HDAC9", "HDAC10")

# Convert HGNC symbols to Entrez IDs with org.Hs.eg.db
# 1-1 mapping, great!!
hdac_entrez_ids <- AnnotationDbi::mapIds(org.Hs.eg.db, keys = hdac_genes,
                     column = "ENTREZID", keytype = "SYMBOL", multiVals = "first")
hdac_entrez_ids <- tibble::tibble(
  HGNC_SYMBOL = names(hdac_entrez_ids),
  GENEID = hdac_entrez_ids)

# retrieve start and end positions of hdac chromosoms
txdb <- AnnotationDbi::loadDb("data/genome_builds/human_gencode_v42.sqlite")
hdac_coords <- AnnotationDbi::select(txdb,
                     keys = hdac_entrez_ids$GENEID,
                     columns = c("TXCHROM", "TXSTART", "TXEND"),
                     keytype = "GENEID")

# take the min and max positions for all chromosomes
hdac_coords <- hdac_coords |>
  dplyr::group_by(GENEID, TXCHROM) |>
  dplyr::summarise(TXSTART=min(TXSTART), TXEND=max(TXEND)) |>
  dplyr::ungroup() |>
  dplyr::inner_join(hdac_entrez_ids, by = "GENEID") |>
  dplyr::arrange(GENEID) |>
  relocate(HGNC_SYMBOL)
readr::write_csv(hdac_coords,
                 "data/gene_positions/hdac_coords.csv")

# hdac_coords <- readr::read_csv("data/gene_positions/hdac_coords.csv", 
#                               show_col_types = FALSE)
flextable(hdac_coords) |> 
  bold(bold = TRUE, part = "header")
```

- [**Remark**: Instead of fetching start and end locations automatically, alternative would have consisted of adding an `INFO/GENE` field within the `VCF` files.]{fg="red"}

##### ii) Extract SNPs corresponding to provided HDAC gene

-   **Inputs**:
    -   Cleaned **VCF** file of the chromosome 7 (where `HDAC-9` is present)^[All curated VCF files per chromosome are stored in `./data/genotypages/` (symbolly linked with `/mnt/projects_tn01/Cartagene/genotypage/imputation/imputation_merged`).]. 
    
-   **Outputs**:
    -   `chr7.HDAC9.vcf`: VCF file, restrained to HDAC9 region with affiliated SNPs.
    
-   **Objective**: Extract annotated SNPs within `HDAC9` boundaries, as defined by the `hg38` reference genome. **Details**:
    -   Discard variants with **Minor Allele Frequency (MAF)** $< 0.01$ (in other words, the SNP must be present in at least $1\%$ of the samples).
    -   SNPs were extracted using `bcftools view`. [**Alternative**: `plink2 --bfile your_dataset --chr 7 --from-bp 18086825 --to-bp 19002414`, but requires BIM files as input.]{fg="red"}.


-   **Bash command**:

``` bash
nohup ./shell/extract_variants.sh > shell/extract_variants.log 2>&1 &
```

- **Remark:** `--maf` guarantees removal of highly recessive SNPs, present in less than $1\%$ of cases with respect to the dominant form. Such SNPs are indeed associated with low statistical power.

```{r}
#| lst-label: lst-vcf-header
#| lst-cap: Header lines, marked by `##` symbol, on top of VCF files. This metadata can prove useful to know exactly how SNPs were extracted in a given region.
#| label: vcf-header
#| echo: false


chromosome_vcf6_file <- "../genotypage/imputation/imputation_merged/chr6.merged.clean.noMono.vcf.gz"
chromosome_vcf6_content <- readr::read_lines(chromosome_vcf6_file, n_max = 30)

# show metadata, with tools and information about VCF structure
cat(chromosome_vcf6_content |> 
      stringr::str_subset("^##"), sep = "\n")
```


<!-- #### b) HLA-family {#sec-HLA} -->

<!-- HLA genes are usually classified into **Class I (HLA-A, HLA-B, HLA-C)** and **Class II (HLA-DRB1, HLA-DQA1, HLA-DQB1)**, amounting in total to six main genes within the human organism. -->

<!-- To retrieve their positions on the latest `Hg38` Genome Build in an automated fashion, see @lst-hla-positions. HLA positions are reported in @tbl-hla-positions. [Do not use the `BiomarT` package: it's not anymore maintained, and can only be used with out-of-date `Hg37` Human Genome Build. Report to [Genome Builds Versions](https://www.linkedin.com/posts/%F0%9F%8E%AF-ming-tommy-tang-40650014_genome-builds-matter-avoid-costly-mistakes-activity-7293270985930141698-ZFkH) for details. Besides, note that when several start and end positions were assigned to the same gene, we consider each time the minimal starting position, and the maximal end position.]{fg="red"} -->



<!-- ```{r} -->
<!-- #| label: tbl-hla-positions -->
<!-- #| lst-label: lst-hla-positions -->
<!-- #| lst-cap:  Use `AnnotationDBI` to fetch and retrieve automatically start and end positions of the HLA family.  -->
<!-- #| tbl-cap: "HLA Gene Positions" -->
<!-- # renv::install(packages = c("bioc::AnnotationDbi", "bioc::org.Hs.eg.db")) -->

<!-- # retrieve latest human genome assembly -->
<!-- # hla_genes <- c("HLA-A", "HLA-B", "HLA-C", "HLA-DQA1", "HLA-DQB1", "HLA-DRB1") -->
<!-- # # Convert HGNC symbols to Entrez IDs with org.Hs.eg.db -->
<!-- # # 1-1 mapping, great!! -->
<!-- # hla_entrez_ids <- AnnotationDbi::mapIds(org.Hs.eg.db, keys = hla_genes,  -->
<!-- #                      column = "ENTREZID", keytype = "SYMBOL", multiVals = "first") -->
<!-- # hla_entrez_ids <- tibble::tibble( -->
<!-- #   HGNC_SYMBOL = names(hla_entrez_ids), -->
<!-- #   GENEID = hla_entrez_ids) -->
<!-- #  -->
<!-- # # retrieve start and end positions of HLA chromosoms -->
<!-- # txdb <- AnnotationDbi::loadDb("data/genome_builds/txdb_human_gencode_v42.sqlite") -->
<!-- # hla_coords <- AnnotationDbi::select(txdb,  -->
<!-- #                      keys = hla_entrez_ids$GENEID,  -->
<!-- #                      columns = c("TXCHROM", "TXSTART", "TXEND"),  -->
<!-- #                      keytype = "GENEID") -->
<!-- #  -->
<!-- # # take the min and max positions for all chromosomes -->
<!-- # hla_coords <- hla_coords |>  -->
<!-- #   dplyr::filter(TXCHROM=="chr6") |>  -->
<!-- #   dplyr::group_by(GENEID, TXCHROM) |>  -->
<!-- #   dplyr::summarise(TXSTART=min(TXSTART), TXEND=max(TXEND)) |>  -->
<!-- #   dplyr::ungroup() |>  -->
<!-- #   dplyr::inner_join(hla_entrez_ids, by = "GENEID") |>  -->
<!-- #   dplyr::arrange(GENEID) |>  -->
<!-- #   relocate(HGNC_SYMBOL) -->
<!-- # readr::write_csv(hla_coords,  -->
<!-- #                  "data/gene_positions/hla_coords.csv") -->

<!-- hla_coords <- readr::read_csv("data/gene_positions/hla_coords.csv",  -->
<!--                               show_col_types = FALSE) -->
<!-- flextable(hla_coords) |>  -->
<!--   bold(bold = TRUE, part = "header") -->
<!-- ``` -->


``` bash
bcftools filter --regions chr6:29942470-32666657 --include 'MAF[0] > 0.01' ./data/genotypages/chr6.merged.clean.noMono.vcf.gz -o ./data/variants/chr6.HLA-complex.vcf
```

## Step 6: GWAS

#### i) GLM and GWAS {#sec-GWAS}

-   **Objective**: Use of `PLINK2 + glm` with the first 10 principal components as covariates, see @lst-glm-bash.
    -   **Osteo analysis**: `OSTEOPOROSIS`, `OSTEOPONIA` and `fractures` using logistic regression + `DMOTSCORE_mod` was analysed using `lm`.
    -   **Cardiovascular association analyses**:  using logistic regression.
    
-   **Input**:
    -   VCF file on the region/gene of interest
    -   Phenotypes, with individual patient IDs.
    -   Explanatory variable to predict, provided with `--pheno-name <response_variable>`.

::: {#lst-glm-bash}

**Template GLM instruction for GWAS studies:**

``` bash

plink2 \
  --double-id \ # <1>
  --pheno ./phenotypes/merge_phenos_PCs.txt \ # <2>
  --keep /mnt/projects_tn01/Cartagene/analyses/phenotypes/merge_phenos_PCs.txt \ # <2>
  --pheno-name OSTEOPONIA \ # <2>
  --vcf /mnt/projects_tn01/Cartagene/analyses/variants_extraction/chr7.merged.clean.noMono.extracted_variants.HDAC9.vcf dosage=HDS # <2>
  --glm hide-covar \ # <3>
  --covar /mnt/projects_tn01/Cartagene/analyses/phenotypes/merge_phenos_PCs.txt \ # <3>
  --covar-name PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10 \ # <3>
  --out HDAC9_OSTEOPONIA # <4>
```
1.  `--1` ensure independent GWAS analyses per individual, aka the *single-sample mode*. Indeed, `PLINK` usually assumes samples are structured in a family-based format (FID IID pairs), while `--double-id` details the FID/IID format for uniquely identifying a given individual.
2.  **GWAS inputs**: We need the *phenotype information* (provided with `--pheno` and `--keep` commands), the *response variable* (provided with `--pheno-name`) and the *VCF file* (command `--vcf`), here using the SNPs annotations for the HDAC9 gene[^4].
3.  GWAS model options: `--glm` is the general linear model, which uses by default a *logistic regression* for categorical variables, and a *standard linear Gaussian model*, equivalent to `lm` for continuous variables^[The `hide-covar` option runs GLM, but doesn't output covariate results. Besides, note that --covar passes *quantitative covariates* (e.g., age, PCs), while `--ide-covar` passes *categorical covariates*]. `--covar` is the covariate file, with relevant explanatory variables to integrate reported with `--covar-name` (see @ for details). PCAs are used to describe the population structure in an unsupervised manner; and avoid and detect latent subgroups. 
4.  The output GWAS folder, with `--out` command. Stored for now in `/mnt/projects_tn01/Cartagene/analyses/association`.

[^4]: `dosage=HDS`, for Hard Dosage, provides the instruction describing genotype uncertainties for enhanced statistical power. It's particularly useful when working as here with **imputed genotypes**, where part of the SNPs were inferred using reference panels.

:::

-   **Remark 1**: you may come up with `Error: Cannot proceed with --glm regression on phenotype 'TACAIX', since variance inflation factor for covariate 'PC2' is too high`. In this case, you may try removing completely covariates (with `--glm allow-no-covars`), or/and increase variance inflation threshold (`--vif number_vif` option). **Note that a VIF factor above 10 is considered problematic as it can significantly distort regression estimates.** [Discussion and visualisations on [*VIF influence*](https://www.linkedin.com/feed/update/urn:li:activity:7308463515432833026/). Briefly, VIF quantifies the degree of similarity among predictor variables. Indeed, the GLM-family usually assumes independence between explanatory variables. My premise here is that the whole pipeline described in @sec-preprocessing-redundancy is only used to compute the PCA eigen vectors, and not for GWAS differential downstream analyses. As such, strongly correlated SNPs are kept, inducing logically an inflation of the VIF factor.]{fg="red"}

-   [**Remark 2**: All the pre-processing operations detailed in @sec-preprocessing-redundancy are only used for the computation of the PCA components, but not subsequently used in the `glm` regression for trimming low-quality SNPs, or even remove strongly correlated individuals. Besides, recommended to use original BIM/BAM/FAM files or even `.pgen` newest formats, inducing faster analysis large datasets, rather than VCF files, using the `--pfile` instruction.]{fg="red"}

-   [**Remark 3**: Add phenotype covariates, such as `SEX` or `AGE` in the regression framework, which can play a strong leverage on the impact of SNPs. [`GENESIS`](https://www.bioconductor.org/packages/devel/bioc/vignettes/GENESIS/inst/doc/assoc_test.html#mixed-model-association-testing) is a R package mixing environmental and genetic factors in a *fixed linear model* approach.]{fg="red"}

- **Remark 4**: we provide in @lst-plink2-efficient the recommended and computationally efficient `plink2` calling, assuming as inputs already compressed binary files. 

::: {#lst-plink2-efficient}

``` bash
plink2 --bfile mydata \
  --pheno phenotype.txt \
  --covar covar.txt --covar-name PC1-PC10 \
  --ide-covar covar.txt --covar-name sex,cohort \
  --glm hide-covar
```

:::

#### ii) Visualisations: Manhattan plots

- More customisation available in [Manhattan plot in R: a review](https://r-graph-gallery.com/101_Manhattan_plot.html), and [How I Make QQ Plots Using `ggplot2`?](https://danielroelfs.com/blog/how-i-make-qq-plots-using-ggplot/)
- [How to interpret a $p$-value histogram](https://varianceexplained.org/statistics/interpreting-pvalue-histogram/)

```{r}
#| label: gwas-visu-preparation
# cartagene_response_variables$Features
gwas_plots_hdac <- lapply(cartagene_response_variables$Features, 
                          function(feature_label) generate_gwas_per_phenotype (genename = "HDAC-9",
                                                                               pheno_label = feature_label, filedate="2025-04-07")) 

gwas_plots_hdac <- gridExtra::marrangeGrob(gwas_plots_hdac, nrow=1, ncol=1)

ggsave("figures/gwas_HDAC9.pdf",
       gwas_plots_hdac, dpi = 600,
       width = 8, height = 12) 
```

#### iii) Multiple test correction 

In total, **7 phenotypes, 2613 variants tested, and $18291$ pairwise tests** were performed, implying to correct for multiple test. A significance threshold of $2.7 \times 10^{-6}$ is suggested (for the raw $p-$ value).

Perspective improvements: 

- The details for the computation of the adjusted $p-$ values are not reported, report to improved approaches here: 

  - [Recent Adjusted $p$-value correction method](https://www.linkedin.com/posts/adrianolszewski_statistic-datascience-research-activity-7301605962820284417-ho3Z). 
  - [Multiple Comparisons Using R](https://www.taylorfrancis.com/books/mono/10.1201/9781420010909/multiple-comparisons-using-frank-bretz-torsten-hothorn-peter-westfall), comprehensive resource of distinct R package strategies for correcting for multiplicity of $p$-values, from @bretz2016. Report also to @nte-MTC for an overview of existing approaches. 

::: {#nte-MTC .callout-note title = "Classes of Multiple Test correction approaches" collapse = "true"}

**Multiple comparison procedures** (MCPs) can be classified into the following adjustment methods:

1. *Fixed sequential*: no adjustment as long as H0s are rejected. One non-rejection and it stops testing with all others non-rejected.

2. *Fallback*: significance level (e.g. 0.05) is split between all hypotheses (can be further weighted). With rejection of the subsequent H0s, their fractions of significance level is accumulated and passed to the next comparison. 

3. *Gatekeeper*: serial, parallel & combined trees, collecting hypotheses into families. Groups of testing can be interpreted as *gatekeepers*: only if any $H_0$ in a family is rejected that the significance is propagated to the next family. The approach is the most versatile, and can be flexibly implemented in R package [`gMCPLite`](https://merck.github.io/gMCPLite). See also [Graphical Approaches
to Multiple Test Problems](https://baselbiometrics.github.io/home/docs/talks/20220329/2_Glimm_Bretz_Xi.pdf), for additional details.

:::
  
- Add **QQplots** and **Manhattan plots**, to visualise the distribution of p-values.

# Perspectives

## Beyond GWAS analyses: retrieve automatically variants of interest {#sec-post-GWAS}

This analysis is not formally a GWAS study, but rather a *gene-centric* study, pre-selecting genes based on prior knowledge and/or intuition.

Alternatively, we could consider a data-driven, and more agnostic approach, to refine the selection of gene candidates. Several approaches to that end have been implemented:

- @gnanaolivu2025bb proposed in [A clinical knowledge graph-based framework to prioritize candidate genes for facilitating diagnosis of Mendelian diseases and rare genetic conditions](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-025-06096-2), the `phenotype prioritization and analysis for rare diseases, PPAR` algorithm to rank genes based on *human phenotype ontology (HPO)* terms.[^5]. In details, `PPAR` combines embeddings from the *human knowledge graph*, incorporating genes, HPO terms, and gene ontology annotations connections. For each input HPO term, a prioritized list of genes is returned based on their relevance and similarity to the HPO term.
- @bridges2025bb developped in [Towards a standard benchmark for phenotype-driven variant and gene prioritisation algorithms: `PhEval`, Phenotypic inference Evaluation framework](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-025-06105-4) a unified benchmarking platform to clean inputs for *phenotype-driven VGPAs*. Variant and Gene Prioritisation Algorithms integrate complex and multi-modal datasets, such as ontologies and gene-to-phenotype associations, to predict the most influential and promising gene targets controlling the evolution of rare diseases. Model is available as GitHub Repo [`PhEval`](https://github.com/monarch-initiative/pheval).

[^5]: Note that relying on HPO terms, for example, `HP:0004322 – Osteopenia`, would alleviate the need for standardising phenotype labels, using universally acknowledged scientific terms instead.

## Incorporation of familial pedigrees and/or environmental factors

-   [Genome-wide association study of fat content and fatty acid composition of shea tree.](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-025-11344-z), from @attikora2025bg. Under the hood, relies on the `mrMLM` R package to include both the *population structure matrix* and the *kinship matrix*. Provides a complete GWAS pipeline, including a number of meaningful illustrations.

-   [`BHCox`: Bayesian heredity-constrained Cox proportional hazards models for detecting gene-environment interactions](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-025-06077-5), from @sun2025bb. Under the hood, relies on the `brms` R package.

## Inference of polygenic scores

- [`XPRS`: A Tool for Interpretable and Explainable Polygenic Risk Score](10.1093/bioinformatics/btaf143), from @kim2025b, is a R package for inferring the *polygenic risk score* for assessing genetic susceptibility to diseases, providing additional interpretation and visualisation tools.  PRSs are further split and classified into genes and single nucleotide polymorphism (SNP) contribution scores via *Shapley additive explanations* (SHAPs), visualised as **Manhattan plots**, **LocusZoom-like** plots and tables at the population and individual levels.

-   [`PNL`: a software to build polygenic risk scores using a Super Learner approach based on `PairNet`, a Convolutional Neural Network](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btaf071/8015614), from @chen2025b.

## Multimodal integration

- @zhao2025bib implements [`SC-VAR`: a computational tool for interpreting polygenic disease risks using single-cell epigenomic data](https://academic.oup.com/bib/article/26/2/bbaf123/8092303?searchresult=1&login=false), a novel computational tool available as a GitHub repo and Python application: [`SC-VAR` GitHub](https://github.com/gefeiZ/SC_VAR). `SC-VAR` uses single-cell *epigenomic* data to predict functional outcomes of the identified disease-associated *GWAS variants*, enhancing their interpretability. Under the hood, relies on [`MAGMA` linear modelling framework](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004219), developed by @leeuw2015pcb.

# Appendix {.appendix}

## Deal with large files committed unwillingly

``` bash 

python3 bin/git-filter-repo --path data/variants/chr6.HLA-complex.vcf --invert-paths

```

## Load CLI `bcftools` and `plink2`

``` bash

export PATH=$PATH:/mnt/software/singularity/bcftools OU bcftools-1.9.sif # <1>

export SINGULARITY_BIND=/mnt/projects_tn01/Cartagene/cartagene-gwas
singularity exec --bind $SINGULARITY_BIND:/data my_container.sif 

singularity pull docker://quay.io/biocontainers/bcftools:1.21--h3a4d415_1 
singularity exec bcftools.sif bcftools --version


singularity pull biocontainers/bcftools
```




## Bibliographic References {.appendix}

::: {#refs}
:::

<!-- ## Bash commands to retrieve files' content matching a string pattern, and files extensions {.appendix} -->

<!-- ``` bash -->
<!-- find . -type f \( -iname "*.sh" -o -iname "*.py" -o -iname "*.Rmd" -o -iname "*.qmd" -o -iname "*.txt" \) -exec grep -Hn "genotypage" {} \; -->

<!-- find . -type f -name "*.sas7bdat" -->
<!-- ``` -->
